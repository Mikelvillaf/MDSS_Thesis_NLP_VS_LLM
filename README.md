# Predicting Helpfulness of Amazon Customer Reviews

This project explores the prediction of review helpfulness on Amazon using various machine learning models and large language models (LLMs). It compares traditional feature-based ML classifiers (like Random Forest, SVM, and Gradient Boosting), deep learning approaches (CNN/RCNN), and LLM-based classification using GPT and DeepSeekAI.

The experiment uses a subset of the [Amazon Reviews 2023 dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023), which contains over 570 million customer reviews from 1996 to 2023.

## ðŸ” Objective

To determine whether a customer review is **helpful** or **unhelpful** using different modeling approaches. We benchmark the performance of:

- Structured + NLP features with classical ML models
- Deep learning models trained directly on review text
- LLMs used as zero-shot classifiers via API

## âš™ï¸ Project Structure
amazon-helpfulness/
â”œâ”€â”€ data/                 # Local review/meta files (not tracked by Git)
â”œâ”€â”€ scripts/              # All processing and modeling logic
â”œâ”€â”€ tests/                # Unit tests for reproducibility
â”œâ”€â”€ tracking/
â”œâ”€â”€ configs/              # Configs for experiments and pipelines
â”œâ”€â”€ results/              # Output: metrics, predictions, figures
â”œâ”€â”€ main.py               # Orchestrates full experiment runs
â””â”€â”€ requirements.txt      # Python dependencies

## ðŸ“¦ Dataset

We use the [Amazon Reviews 2023 dataset](https://github.com/hyp1231/AmazonReviews2023) by McAuley Lab.

Selected product categories include:

- `Books`
- `CDs and Vinyl`
- `Clothing, Shoes, and Jewelry`
- `Beauty and Personal Care`
- `Cell Phones and Accessories`
- `Home and Kitchen`

Each review record includes:

- `rating` (float)
- `title` and `text` (str)
- `helpful_votes` (int)
- `verified_purchase` (bool)
- `timestamp` (int)
- `asin`, `parent_asin`, `user_id` (str)

Metadata includes:

- `price`, `average_rating`, `features`, `description`
- `images`, `store`, `brand`, and category hierarchies

## ðŸ§ª Labeling Strategy

A binary label is assigned to each review:

- **Helpful**: if `helpful_votes / total_votes_on_product` â‰¥ `0.75`
- **Unhelpful**: if â‰¤ `0.35`
- All others are discarded to reduce label noise

Reviews with zero votes are excluded.

## ðŸ“Š Evaluation Metrics

Models are compared using:

- **F1 Score**
- **ROC-AUC**

- **Inference Time / Cost** (esp. for LLMs)

Experiments are run across multiple seeds and subsets for robustness.

## Tracking with Weave (Weights & Biases)

This project uses [Weave](https://wandb.ai/weave) from Weights & Biases to track data processing and model performance.

To start tracking:

1. Make sure you're logged in with `wandb login`
2. The pipeline auto-initializes tracking on first run via:

```python
from tracking.wandb_init import init_tracking
init_tracking("amazon-helpfulness")

## ðŸ’¡ Reproducibility

- Modular pipeline with `main.py`
- Seed control via `configs/experiment_config.yaml`
- Unit tests in `tests/`
- Logging and output saved to `results/`

## ðŸ“„ Citation

If you use the Amazon Reviews dataset, please cite:

> Hou, Yupeng, et al. *Bridging Language and Items for Retrieval and Recommendation.* arXiv preprint arXiv:2403.03952, 2024.

## ðŸ§  Credits

Developed by [Mikel Villalabeitia](https://github.com/Mikelvillaf)  
Dataset by McAuley Lab, UCSD


## ðŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/Mikelvillaf/MDSS_Thesis_NLP_VS_LLM.git
cd MDSS_Thesis_NLP_VS_LLM

