# Configuration file for the Review Helpfulness Prediction Pipeline

# --- General Settings ---
random_seeds: [42] #, 123, 2025] # List of random seeds for ensuring reproducibility across runs. One run per seed.

# --- Data Source ---

# Set to a number (e.g., 1000000) to limit, or null/omit to load all.
max_initial_rows_per_category: 1000000 # Example: Load up to 1 million rows before any filtering

categories:
  - CDs_and_Vinyl # Example category
  # - Books
  # - Home_and_Kitchen
  # - Beauty_and_Personal_Care
  - Digital_Music
  # - Cell_Phones_and_Accessories


year_range: [2010, 2023] # Inclusive range [start_year, end_year] to filter reviews by timestamp.

# --- Labeling Strategy ---
labeling:
  mode: "percentile" # 'threshold' or 'percentile'
  helpful_ratio_min: 0.75
  unhelpful_ratio_max: 0.25
  top_percentile: 0.25    # Used if mode: 'percentile'
  bottom_percentile: 0.25 # Used if mode: 'percentile'
  min_total_votes: 1      # Min votes for a review's product to be considered for labeling.
  use_length_filter: true
  min_review_words: 15
  max_review_words: 750 # Consider aligning with paper (e.g., 500)

# --- Temporal Data Splitting ---
temporal_split_years:
  train_years: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
  val_year: 2020
  test_year: 2021

# --- Data Sampling Strategy ---
balanced_sampling:
  use_strict_balancing: true # Target 50/50 splits for ML/DL train/val/test
  samples_per_class:
    train: 40000
    val: 5000
    test: 5000
  max_total_samples_imbalanced: # Used if use_strict_balancing: false
    train: null
    val: null
    test: null

# --- Feature Engineering Settings ---
feature_set: hybrid       # 'structured', 'nlp', 'hybrid' (for ML/Hybrid DL)
text_max_features: 1000   # For TF-IDF

# --- Deep Learning Specific Settings ---
dl_feature_set: hybrid    # 'text', 'hybrid' (for DL input)
dl_num_structured_features: 5 # Expected count if dl_feature_set: 'hybrid'
dl_max_words: 10000
dl_max_len: 350
dl_embedding_dim: 64
dl_epochs: 10
dl_batch_size: 64

# --- Model Selection ---
models_to_run:
  ml: ['svm'] # ['svm','random_forest','gradient_boosting']
  dl: [] #'cnn'] # ['cnn','rcnn']
  llm_openai: # Replace with your actual OpenAI model ID
    # - "gpt-4o-mini" 
    #- 
  llm_google: # valid Gemini model ID
    # - "gemini-2.0-flash-lite" 
    #-

# --- LLM Evaluation Specific Configuration ---
llm_evaluation:
  openai_api_key_env_var: "OPENAI_API_KEY" # provider_api_key_env_var 
  google_api_key_env_var: "GOOGLE_API_KEY" # 
  # anthropic_api_key_env_var: "ANTHROPIC_API_KEY" # Example for future LLM implementations
  test_sample_size: 50 # null if running full prediction
  prompting_modes: ['zero_shot', 'few_shot']
  request_timeout: 30 # This will be passed to the wrapper's generate method
  max_retries: 3
  retry_delay: 5      # Initial delay (increases exponentially on retries)

  # -- Zero-Shot Configuration --
  zero_shot_prompt_template: |
    You are an expert helpfulness classifier for online customer reviews.
    Definition of Helpful (1): "A helpful review provides specific details about the product's features, performance, or usage context. It often includes clear pros and cons, compares the product to alternatives, offers unique insights, or justifies its rating with concrete examples. The review helps a potential buyer make an informed decision."
    Definition of Unhelpful (0): "An unhelpful review is often vague, overly brief, purely emotional without justification, irrelevant to the product itself (e.g., complaining only about shipping), or contains nonsensical text. It does not provide useful information for a potential buyer."

    Classify the following review text as either '1' (Helpful) or '0' (Unhelpful) based on the definitions provided.
    Respond with ONLY the number '1' or '0'. Do not add any other text or punctuation.

    Review Text:
    ---
    {review_text}
    ---

    Classification (1 or 0):

  # -- Few-Shot Configuration --
  few_shot:
    num_examples: 2 # Number of examples from training set.
    # Selection strategy from training data:
    # 'balanced_random', 'random', 'extreme_helpful_vote'
    example_selection_strategy: 'extreme_helpful_vote'

    # Format string for each selected example shown to the LLM (using 1/0).
    example_format: |
      "{review_text}" = {label_text}

    # Main prompt template for few-shot (using 1/0).
    prompt_template: |
      You are an expert helpfulness classifier for online customer reviews.
      Use the following definitions and examples to classify the final review text.
      Definition of Helpful (1): "A helpful review provides specific details about the product's features, performance, or usage context. It often includes clear pros and cons, compares the product to alternatives, offers unique insights, or justifies its rating with concrete examples. The review helps a potential buyer make an informed decision."
      Definition of Unhelpful (0): "An unhelpful review is often vague, overly brief, purely emotional without justification, irrelevant to the product itself (e.g., complaining only about shipping), or contains nonsensical text. It does not provide useful information for a potential buyer."

      Classify the final review text as either '1' (Helpful) or '0' (Unhelpful) based on the definitions and examples provided.
      Respond with ONLY the number '1' or '0' for the final review. Do not add any other text or punctuation.

      Examples:
      ---
      {examples}
      ---

      Now classify this review:
      Review Text:
      ---
      {review_text}
      ---

      Classification (1 or 0):

# --- Output Directory ---
output_dir: results/ # Base directory for saving run results.